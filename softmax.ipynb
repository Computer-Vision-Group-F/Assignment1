{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Assignment 1-3: Softmax\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def get_CIFAR10_data():\n",
    "    # Load CIFAR-10 data\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # Normalize the data (pixel values between 0 and 1)\n",
    "    X_train = X_train.astype('float32') / 255\n",
    "    X_test = X_test.astype('float32') / 255\n",
    "\n",
    "    # Convert class vectors to binary class matrices (one-hot encoding)\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Optionally, further split into a development set\n",
    "    X_val, X_dev, y_val, y_dev = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "# Invoke the function to get our data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "\n",
    "# Print shapes for verification\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape: ', X_dev.shape)\n",
    "print('Development labels shape: ', y_dev.shape)\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 images from X_train:\n",
      " [[[[0.13333334 0.14117648 0.16862746]\n",
      "   [0.14509805 0.14509805 0.2       ]\n",
      "   [0.17254902 0.17254902 0.2509804 ]\n",
      "   ...\n",
      "   [0.24313726 0.28627452 0.3647059 ]\n",
      "   [0.20392157 0.24705882 0.3254902 ]\n",
      "   [0.1764706  0.22352941 0.3019608 ]]\n",
      "\n",
      "  [[0.13725491 0.13725491 0.14509805]\n",
      "   [0.13333334 0.13333334 0.15686275]\n",
      "   [0.13333334 0.12941177 0.16862746]\n",
      "   ...\n",
      "   [0.22745098 0.27058825 0.34901962]\n",
      "   [0.19607843 0.23529412 0.3137255 ]\n",
      "   [0.18431373 0.22745098 0.30588236]]\n",
      "\n",
      "  [[0.13333334 0.12941177 0.13725491]\n",
      "   [0.12941177 0.1254902  0.14117648]\n",
      "   [0.12941177 0.1254902  0.14901961]\n",
      "   ...\n",
      "   [0.25882354 0.3019608  0.38039216]\n",
      "   [0.2        0.24313726 0.32156864]\n",
      "   [0.20784314 0.2509804  0.32941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.1254902  0.1254902  0.14901961]\n",
      "   [0.1254902  0.12156863 0.14509805]\n",
      "   [0.12941177 0.12941177 0.14901961]\n",
      "   ...\n",
      "   [0.16078432 0.14901961 0.1764706 ]\n",
      "   [0.13333334 0.12156863 0.14901961]\n",
      "   [0.14117648 0.1254902  0.15686275]]\n",
      "\n",
      "  [[0.12941177 0.12156863 0.14117648]\n",
      "   [0.1254902  0.11764706 0.14117648]\n",
      "   [0.13333334 0.1254902  0.14509805]\n",
      "   ...\n",
      "   [0.14509805 0.13725491 0.15686275]\n",
      "   [0.1254902  0.11372549 0.13725491]\n",
      "   [0.11764706 0.10980392 0.12941177]]\n",
      "\n",
      "  [[0.14509805 0.1254902  0.13725491]\n",
      "   [0.14509805 0.1254902  0.14117648]\n",
      "   [0.14509805 0.12941177 0.14117648]\n",
      "   ...\n",
      "   [0.19215687 0.18431373 0.19215687]\n",
      "   [0.18431373 0.1764706  0.1882353 ]\n",
      "   [0.22745098 0.21960784 0.22745098]]]\n",
      "\n",
      "\n",
      " [[[0.35686275 0.36862746 0.35686275]\n",
      "   [0.37254903 0.44313726 0.34509805]\n",
      "   [0.36078432 0.4745098  0.35686275]\n",
      "   ...\n",
      "   [0.5882353  0.6        0.5882353 ]\n",
      "   [0.5137255  0.5686275  0.41568628]\n",
      "   [0.34901962 0.4627451  0.14901961]]\n",
      "\n",
      "  [[0.39215687 0.53333336 0.39607844]\n",
      "   [0.36862746 0.5019608  0.39215687]\n",
      "   [0.27450982 0.40392157 0.2784314 ]\n",
      "   ...\n",
      "   [0.3019608  0.30980393 0.30588236]\n",
      "   [0.3882353  0.44705883 0.3529412 ]\n",
      "   [0.3372549  0.43137255 0.22352941]]\n",
      "\n",
      "  [[0.40784314 0.5647059  0.3882353 ]\n",
      "   [0.33333334 0.47843137 0.33333334]\n",
      "   [0.22745098 0.32941177 0.2       ]\n",
      "   ...\n",
      "   [0.24313726 0.2901961  0.27058825]\n",
      "   [0.21960784 0.27450982 0.21960784]\n",
      "   [0.2627451  0.33333334 0.22352941]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.4627451  0.41960785 0.40392157]\n",
      "   [0.5803922  0.54509807 0.5568628 ]\n",
      "   [0.52156866 0.47058824 0.45882353]\n",
      "   ...\n",
      "   [0.32156864 0.2901961  0.35686275]\n",
      "   [0.4862745  0.42745098 0.46666667]\n",
      "   [0.41960785 0.3529412  0.36862746]]\n",
      "\n",
      "  [[0.56078434 0.47843137 0.49019608]\n",
      "   [0.5882353  0.5254902  0.54901963]\n",
      "   [0.70980394 0.61960787 0.59607846]\n",
      "   ...\n",
      "   [0.2901961  0.27450982 0.3372549 ]\n",
      "   [0.43137255 0.4        0.41568628]\n",
      "   [0.38039216 0.3529412  0.34509805]]\n",
      "\n",
      "  [[0.4745098  0.43529412 0.42745098]\n",
      "   [0.5294118  0.49411765 0.49803922]\n",
      "   [0.60784316 0.5411765  0.53333336]\n",
      "   ...\n",
      "   [0.25490198 0.22352941 0.2901961 ]\n",
      "   [0.50980395 0.4627451  0.4745098 ]\n",
      "   [0.5764706  0.53333336 0.5254902 ]]]\n",
      "\n",
      "\n",
      " [[[0.8156863  0.76862746 0.61960787]\n",
      "   [0.7764706  0.7019608  0.5921569 ]\n",
      "   [0.6431373  0.60784316 0.50980395]\n",
      "   ...\n",
      "   [0.68235296 0.62352943 0.5254902 ]\n",
      "   [0.7294118  0.6745098  0.5294118 ]\n",
      "   [0.7529412  0.73333335 0.5764706 ]]\n",
      "\n",
      "  [[0.8        0.7647059  0.61960787]\n",
      "   [0.7019608  0.64705884 0.5529412 ]\n",
      "   [0.6        0.56078434 0.5058824 ]\n",
      "   ...\n",
      "   [0.68235296 0.61960787 0.5372549 ]\n",
      "   [0.74509805 0.68235296 0.56078434]\n",
      "   [0.74509805 0.70980394 0.56078434]]\n",
      "\n",
      "  [[0.7254902  0.7058824  0.5803922 ]\n",
      "   [0.5294118  0.49411765 0.4392157 ]\n",
      "   [0.39215687 0.34901962 0.36078432]\n",
      "   ...\n",
      "   [0.7490196  0.6901961  0.5882353 ]\n",
      "   [0.7058824  0.6392157  0.5294118 ]\n",
      "   [0.7882353  0.7372549  0.6117647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8509804  0.827451   0.81960785]\n",
      "   [0.73333335 0.6901961  0.7372549 ]\n",
      "   [0.69411767 0.6431373  0.6666667 ]\n",
      "   ...\n",
      "   [0.54509807 0.43137255 0.4627451 ]\n",
      "   [0.5882353  0.4392157  0.4745098 ]\n",
      "   [0.84705883 0.7764706  0.77254903]]\n",
      "\n",
      "  [[0.93333334 0.89411765 0.8862745 ]\n",
      "   [0.8862745  0.8392157  0.8509804 ]\n",
      "   [0.91764706 0.8745098  0.8666667 ]\n",
      "   ...\n",
      "   [0.5921569  0.44705883 0.47843137]\n",
      "   [0.7176471  0.58431375 0.60784316]\n",
      "   [0.85490197 0.7764706  0.78039217]]\n",
      "\n",
      "  [[0.9254902  0.8784314  0.87058824]\n",
      "   [0.84705883 0.8        0.8       ]\n",
      "   [0.88235295 0.8392157  0.81960785]\n",
      "   ...\n",
      "   [0.73333335 0.6313726  0.6392157 ]\n",
      "   [0.7372549  0.64705884 0.64705884]\n",
      "   [0.69411767 0.61960787 0.6156863 ]]]\n",
      "\n",
      "\n",
      " [[[0.5803922  0.40392157 0.28627452]\n",
      "   [0.58431375 0.4117647  0.27450982]\n",
      "   [0.57254905 0.40392157 0.2627451 ]\n",
      "   ...\n",
      "   [0.44313726 0.26666668 0.1882353 ]\n",
      "   [0.47843137 0.28627452 0.19607843]\n",
      "   [0.4862745  0.29411766 0.18039216]]\n",
      "\n",
      "  [[0.6117647  0.42352942 0.29803923]\n",
      "   [0.5764706  0.3882353  0.25882354]\n",
      "   [0.54509807 0.3647059  0.22745098]\n",
      "   ...\n",
      "   [0.46666667 0.2784314  0.1764706 ]\n",
      "   [0.47843137 0.2784314  0.1764706 ]\n",
      "   [0.46666667 0.2784314  0.16862746]]\n",
      "\n",
      "  [[0.6313726  0.43137255 0.30588236]\n",
      "   [0.5921569  0.38431373 0.25490198]\n",
      "   [0.5647059  0.36078432 0.21568628]\n",
      "   ...\n",
      "   [0.49019608 0.3019608  0.18431373]\n",
      "   [0.5058824  0.3254902  0.22352941]\n",
      "   [0.49019608 0.33333334 0.24705882]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.57254905 0.47843137 0.41960785]\n",
      "   [0.53333336 0.41568628 0.3372549 ]\n",
      "   [0.52156866 0.39607844 0.32156864]\n",
      "   ...\n",
      "   [0.91764706 0.6784314  0.5529412 ]\n",
      "   [0.9372549  0.7019608  0.5764706 ]\n",
      "   [0.8156863  0.6        0.45490196]]\n",
      "\n",
      "  [[0.627451   0.54901963 0.50980395]\n",
      "   [0.6627451  0.56078434 0.5137255 ]\n",
      "   [0.67058825 0.54901963 0.48235294]\n",
      "   ...\n",
      "   [0.9098039  0.6784314  0.5568628 ]\n",
      "   [0.9098039  0.7058824  0.5764706 ]\n",
      "   [0.7882353  0.60784316 0.44705883]]\n",
      "\n",
      "  [[0.6039216  0.5294118  0.49803922]\n",
      "   [0.627451   0.54509807 0.5137255 ]\n",
      "   [0.6666667  0.5647059  0.5254902 ]\n",
      "   ...\n",
      "   [0.84313726 0.62352943 0.49803922]\n",
      "   [0.8        0.59607846 0.46666667]\n",
      "   [0.7921569  0.5921569  0.44705883]]]\n",
      "\n",
      "\n",
      " [[[0.6039216  0.1882353  0.03137255]\n",
      "   [0.5764706  0.15686275 0.01960784]\n",
      "   [0.5686275  0.13725491 0.00784314]\n",
      "   ...\n",
      "   [0.7607843  0.36078432 0.07843138]\n",
      "   [0.7647059  0.36078432 0.07843138]\n",
      "   [0.7647059  0.3647059  0.07450981]]\n",
      "\n",
      "  [[0.69803923 0.42745098 0.23137255]\n",
      "   [0.654902   0.34509805 0.16470589]\n",
      "   [0.6117647  0.25882354 0.09019608]\n",
      "   ...\n",
      "   [0.7490196  0.34901962 0.08235294]\n",
      "   [0.7490196  0.34901962 0.08627451]\n",
      "   [0.7372549  0.3254902  0.07450981]]\n",
      "\n",
      "  [[0.79607844 0.63529414 0.4117647 ]\n",
      "   [0.7647059  0.5921569  0.3529412 ]\n",
      "   [0.7294118  0.5137255  0.2784314 ]\n",
      "   ...\n",
      "   [0.7294118  0.33333334 0.07450981]\n",
      "   [0.7372549  0.33333334 0.07843138]\n",
      "   [0.7372549  0.3254902  0.07843138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.14117648 0.16470589 0.1254902 ]\n",
      "   [0.16862746 0.19215687 0.14509805]\n",
      "   [0.19607843 0.22352941 0.16470589]\n",
      "   ...\n",
      "   [0.10588235 0.19215687 0.14509805]\n",
      "   [0.19607843 0.33333334 0.24705882]\n",
      "   [0.39607844 0.57254905 0.47843137]]\n",
      "\n",
      "  [[0.26666668 0.2901961  0.23137255]\n",
      "   [0.29411766 0.33333334 0.26666668]\n",
      "   [0.2784314  0.32156864 0.25882354]\n",
      "   ...\n",
      "   [0.10588235 0.20392157 0.15294118]\n",
      "   [0.14901961 0.27450982 0.19607843]\n",
      "   [0.27450982 0.45490196 0.34509805]]\n",
      "\n",
      "  [[0.44313726 0.4745098  0.3882353 ]\n",
      "   [0.36862746 0.40784314 0.34901962]\n",
      "   [0.32941177 0.3764706  0.32941177]\n",
      "   ...\n",
      "   [0.07058824 0.13725491 0.10980392]\n",
      "   [0.07450981 0.15294118 0.11372549]\n",
      "   [0.18039216 0.28627452 0.20784314]]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 images from X_train:\\n\", X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Plot the first 5 images\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.axis('off') \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your need to complete `softmax_loss_naive`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    num_train = X.shape[0]  # Number of training samples (N)\n",
    "    num_classes = W.shape[1]  # Number of classes (C)\n",
    "\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)  # Compute the score for each class\n",
    "        scores -= np.max(scores)  # For numeric stability, subtract max from scores\n",
    "\n",
    "        # Calculate softmax probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "        # Loss: -log of the probability of the correct class\n",
    "        correct_class_prob = probs[y[i]]\n",
    "        loss += -np.log(correct_class_prob)\n",
    "\n",
    "        # Gradient computation\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                dW[:, j] += (probs[j] - 1) * X[i]  # Gradient for correct class\n",
    "            else:\n",
    "                dW[:, j] += probs[j] * X[i]  # Gradient for incorrect classes\n",
    "\n",
    "    # Average the loss and gradients\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Add regularization to the loss and gradient\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = y_dev.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = y_dev.astype(int)  # Ensure the labels are integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_dev before: (50000,)\n",
      "Shape of y_dev after: (50000,)\n",
      "loss: 2.301392\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# Check shape and ensure labels are in correct format\n",
    "print('Shape of y_dev before:', y_dev.shape)\n",
    "y_dev = y_dev.reshape(-1).astype(int)\n",
    "print('Shape of y_dev after:', y_dev.shape)\n",
    "\n",
    "# Run the softmax loss function\n",
    "loss, grad = softmax_loss_naive(W, X_dev_flat_with_bias, y_dev, 0.0)\n",
    "\n",
    "# Print the loss for sanity check\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a batch of images in shape (N, H, W, C)\n",
    "# Flatten the images to (N, D) where D = H * W * C\n",
    "X_dev_flat = X_dev.reshape(X_dev.shape[0], -1)  # This will flatten the last three dimensions\n",
    "# Add bias term to the flattened input if necessary\n",
    "X_dev_flat_with_bias = np.hstack([X_dev_flat, np.ones((X_dev_flat.shape[0], 1))])  # shape (N, D+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302084\n"
     ]
    }
   ],
   "source": [
    "# Generate a random softmax weight matrix\n",
    "W = np.random.randn(3073, 10) * 0.0001  # Adjust based on your features\n",
    "\n",
    "# Now run the softmax loss function\n",
    "loss, grad = softmax_loss_naive(W, X_dev_flat_with_bias, y_dev, 0.0)\n",
    "\n",
    "# Check loss\n",
    "print('loss: %f' % loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming W, X_dev, and y_dev are already defined\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute the loss and gradient without regularization\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m softmax_loss_naive(W, X_dev, y_dev, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print the loss for sanity check\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss without regularization: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m loss)\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36msoftmax_loss_naive\u001b[1;34m(W, X, y, reg)\u001b[0m\n\u001b[0;32m     30\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Number of classes (C)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train):\n\u001b[1;32m---> 33\u001b[0m     scores \u001b[38;5;241m=\u001b[39m X[i]\u001b[38;5;241m.\u001b[39mdot(W)  \u001b[38;5;66;03m# Compute the score for each class\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     scores \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(scores)  \u001b[38;5;66;03m# For numeric stability, subtract max from scores\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Calculate softmax probabilities\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Assuming W, X_dev, and y_dev are already defined\n",
    "\n",
    "# Compute the loss and gradient without regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Print the loss for sanity check\n",
    "print('Loss without regularization: %f' % loss)\n",
    "\n",
    "# Numeric gradient checking\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Compute the loss and gradient with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "\n",
    "# Print the loss for sanity check with regularization\n",
    "print('Loss with regularization: %f' % loss)\n",
    "\n",
    "# Numeric gradient checking with regularization\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Complete the implementation of softmax_loss_naive and implement a (naive)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# version of the gradient that uses nested loops.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m softmax_loss_naive(W, X_dev, y_dev, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# As we did for the SVM, use numeric gradient checking as a debugging tool.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# The numeric gradient should be close to the analytic gradient.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradient_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grad_check_sparse\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36msoftmax_loss_naive\u001b[1;34m(W, X, y, reg)\u001b[0m\n\u001b[0;32m     30\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Number of classes (C)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train):\n\u001b[1;32m---> 33\u001b[0m     scores \u001b[38;5;241m=\u001b[39m X[i]\u001b[38;5;241m.\u001b[39mdot(W)  \u001b[38;5;66;03m# Compute the score for each class\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     scores \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(scores)  \u001b[38;5;66;03m# For numeric stability, subtract max from scores\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Calculate softmax probabilities\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    num_train = X.shape[0]  # Number of training examples\n",
    "    num_classes = W.shape[1]  # Number of classes\n",
    "\n",
    "    # Compute scores for all classes\n",
    "    scores = X.dot(W)  # Shape (N, C)\n",
    "    scores -= np.max(scores, axis=1, keepdims=True)  # Numeric stability\n",
    "\n",
    "    # Compute softmax probabilities\n",
    "    exp_scores = np.exp(scores)  # Shape (N, C)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # Shape (N, C)\n",
    "\n",
    "    # Compute the loss: average over the batch and add regularization\n",
    "    loss = -np.sum(np.log(probs[np.arange(num_train), y])) / num_train  # Shape (1,)\n",
    "    loss += reg * np.sum(W * W)  # Regularization loss\n",
    "\n",
    "    # Compute the gradient\n",
    "    dW = X.T.dot(probs)  # Shape (D, C)\n",
    "    # Subtract the gradient for the correct classes\n",
    "    dW[np.arange(W.shape[0]), y] -= X.T.dot(np.ones(num_train))  # Shape (D, C)\n",
    "    dW /= num_train  # Average over the batch\n",
    "    dW += 2 * reg * W  # Regularization gradient\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m loss_naive, grad_naive \u001b[38;5;241m=\u001b[39m softmax_loss_naive(W, X_dev, y_dev, \u001b[38;5;241m0.000005\u001b[39m)\n\u001b[0;32m      7\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive loss: \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m computed in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (loss_naive, toc \u001b[38;5;241m-\u001b[39m tic))\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36msoftmax_loss_naive\u001b[1;34m(W, X, y, reg)\u001b[0m\n\u001b[0;32m     30\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Number of classes (C)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train):\n\u001b[1;32m---> 33\u001b[0m     scores \u001b[38;5;241m=\u001b[39m X[i]\u001b[38;5;241m.\u001b[39mdot(W)  \u001b[38;5;66;03m# Compute the score for each class\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     scores \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(scores)  \u001b[38;5;66;03m# For numeric stability, subtract max from scores\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Calculate softmax probabilities\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,32,3) and (3073,10) not aligned: 3 (dim 2) != 3073 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "# Compute the loss and gradient using the vectorized implementation\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# Compare the results using Frobenius norm for the gradient\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
