{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Assignment 1-2: Support Vector Machine\n",
    "\n",
    "## Multiclass Support Vector Machine exercise\n",
    "\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a fully-vectorized **loss function** for the SVM\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** using numerical gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries requre for run the program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jasha\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jasha\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded and extracted.\n",
      "Training data shape: (50000, 3, 32, 32)\n",
      "Training labels shape: (50000,)\n",
      "Test data shape: (10000, 3, 32, 32)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data &Extract the data from Cifar 10\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Function to show download progress\n",
    "def show_progress(count, block_size, total_size):\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "    print(msg, end='')\n",
    "\n",
    "# Function to download and extract the dataset\n",
    "def download_and_unpack(url, target_dir):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the CIFAR-10 dataset if not already present.\n",
    "\n",
    "    :param url: The URL of the file to download.\n",
    "    :param target_dir: The directory where the file will be saved and extracted.\n",
    "    \"\"\"\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(target_dir, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "\n",
    "        print(f\"Downloading {file_name}...\")\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url, filename=file_path, reporthook=show_progress)\n",
    "        print(\"\\nDownload completed. Extracting files...\")\n",
    "\n",
    "        if file_path.endswith(\".tar.gz\"):\n",
    "            with tarfile.open(name=file_path, mode=\"r:gz\") as tar_ref:\n",
    "                tar_ref.extractall(target_dir)\n",
    "        print(\"Extraction done.\")\n",
    "    else:\n",
    "        print(\"File already downloaded and extracted.\")\n",
    "\n",
    "# Function to load CIFAR-10 batch data\n",
    "def load_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "        X = data_dict[b'data']\n",
    "        Y = data_dict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "# Function to load the CIFAR-10 dataset\n",
    "def load_cifar10(data_dir):\n",
    "    \"\"\"\n",
    "    Loads the entire CIFAR-10 dataset from the given directory.\n",
    "\n",
    "    :param data_dir: The directory where the CIFAR-10 data is located.\n",
    "    :return: Tuple of numpy arrays (X_train, Y_train), (X_test, Y_test)\n",
    "    \"\"\"\n",
    "    # Load all the training batches\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in range(1, 6):\n",
    "        batch_file = os.path.join(data_dir, 'data_batch_' + str(i))\n",
    "        X, Y = load_batch(batch_file)\n",
    "        X_train.append(X)\n",
    "        Y_train.append(Y)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    Y_train = np.concatenate(Y_train)\n",
    "\n",
    "    # Load the test batch\n",
    "    X_test, Y_test = load_batch(os.path.join(data_dir, 'test_batch'))\n",
    "\n",
    "    return (X_train, Y_train), (X_test, Y_test)\n",
    "\n",
    "# Define URL and download directory\n",
    "download_dir = r\"C:\\Users\\jasha\\OneDrive\\cifar-10-batches-py\"  # Replace with your directory\n",
    "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "# Download and extract CIFAR-10 dataset\n",
    "download_and_unpack(url, download_dir)\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = os.path.join(download_dir, 'cifar-10-batches-py')\n",
    "(X_train, Y_train), (X_test, Y_test) = load_cifar10(data_dir)\n",
    "\n",
    "# Check the data shapes\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {Y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "# UPLOAD THE DATA ON ONE DRIVE\n",
    "\n",
    "# Define the download directory and URL\n",
    "download_dir = r\"C:\\Users\\jasha\\OneDrive\\cifar-10-batches-py\"\n",
    "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"  # URL for CIFAR-10 dataset\n",
    "\n",
    "# Call the function to download and extract\n",
    "download_and_unpack(url, download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Start the  Data preprocessing\n",
    "\n",
    "#LOAD CIFAR-10 DATASET\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Check the shape of the loaded data\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min pixel value in x_train: 0.0\n",
      "Max pixel value in x_train: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images to values between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Verify the normalization by printing the minimum and maximum values\n",
    "print(f\"Min pixel value in x_train: {x_train.min()}\")\n",
    "print(f\"Max pixel value in x_train: {x_train.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape after one-hot encoding: (50000, 10)\n",
      "Example of one-hot encoded label: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Performing the one _hot encoding\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "# One-hot encode the labels for training and test datasets\n",
    "y_train = to_categorical(Y_train, num_classes=10)  \n",
    "y_test = to_categorical(Y_test, num_classes=10)   \n",
    "\n",
    "# Check the shape of the one-hot encoded labels\n",
    "print(f\"y_train shape after one-hot encoding: {y_train.shape}\")  \n",
    "print(f\"Example of one-hot encoded label: {y_train[0]}\")  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000, 1)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000, 1)\n",
      "Test data shape:  (1000, 32, 32, 3)\n",
      "Test labels shape:  (1000, 1)\n",
      "Dev data shape:  (500, 32, 32, 3)\n",
      "Dev labels shape:  (500, 1)\n"
     ]
    }
   ],
   "source": [
    "# create a small development set as a subset of the training data;\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Define the number of samples for each subset\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Split the original training data into training and validation sets\n",
    "# Validation set: from num_training to num_training + num_validation\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Training set: first num_training points\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Development set: random subset of training data (500 samples)\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# Test set: first num_test points from the original test set\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Print the shapes of the datasets to verify the splits\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Dev data shape: ', X_dev.shape)\n",
    "print('Dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Preprocessing: subtract the mean image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1: Create the mean image on the base of training data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m mean_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# changed X_train to train_data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_img[:\u001b[38;5;241m10\u001b[39m])  \u001b[38;5;66;03m# print a few of the elements\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing: subtract the mean image\n",
    "# 1: Create the mean image on the base of training data\n",
    "mean_img = np.mean(train_data, axis=0)  # changed X_train to train_data\n",
    "print(mean_img[:10])  # print a few of the elements\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mean_img.reshape((32, 32, 3)).astype('uint8'))  # visualize the mean image\n",
    "plt.show()\n",
    "\n",
    "# 2: Subtract the mean image from training, validation, test, and development data\n",
    "train_data -= mean_img  # changed X_train to train_data\n",
    "val_data -= mean_img  # changed X_val to val_data\n",
    "test_data -= mean_img  # changed X_test to test_data\n",
    "dev_data -= mean_img  # changed X_dev to dev_data\n",
    "\n",
    "# Third: Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "train_data = np.hstack([train_data, np.ones((train_data.shape[0], 1))])  # X_train -> train_data\n",
    "val_data = np.hstack([val_data, np.ones((val_data.shape[0], 1))])  # X_val -> val_data\n",
    "test_data = np.hstack([test_data, np.ones((test_data.shape[0], 1))])  # X_test -> test_data\n",
    "dev_data = np.hstack([dev_data, np.ones((dev_data.shape[0], 1))])  # X_dev -> dev_data\n",
    "\n",
    "print(train_data.shape, val_data.shape, test_data.shape, dev_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean image (first 10 elements): [[[130.71074 136.05614 132.5538 ]\n",
      "  [130.14036 135.44238 131.85358]\n",
      "  [131.05044 136.24616 132.58144]\n",
      "  [131.56886 136.67804 132.8775 ]\n",
      "  [132.1847  137.22858 133.26738]\n",
      "  [132.85184 137.8545  133.78034]\n",
      "  [133.37154 138.28386 134.15504]\n",
      "  [133.89092 138.74072 134.5227 ]\n",
      "  [134.48504 139.27524 135.01422]\n",
      "  [134.9326  139.6326  135.29142]\n",
      "  [135.23398 139.94098 135.51004]\n",
      "  [135.40374 140.0906  135.659  ]\n",
      "  [135.62016 140.26882 135.81572]\n",
      "  [135.63418 140.26172 135.80778]\n",
      "  [135.63402 140.22544 135.7769 ]\n",
      "  [135.61138 140.17206 135.71862]\n",
      "  [135.53046 140.09332 135.58066]\n",
      "  [135.5036  140.06138 135.55468]\n",
      "  [135.49102 140.05346 135.5742 ]\n",
      "  [135.38346 139.96092 135.47278]\n",
      "  [135.10962 139.71384 135.20938]\n",
      "  [134.9583  139.6188  135.177  ]\n",
      "  [134.8102  139.49842 135.10764]\n",
      "  [134.52658 139.29726 134.92798]\n",
      "  [134.04618 138.94646 134.62446]\n",
      "  [133.6069  138.58728 134.33616]\n",
      "  [133.0759  138.10716 133.93838]\n",
      "  [132.32476 137.4154  133.3761 ]\n",
      "  [131.58878 136.7739  132.84264]\n",
      "  [131.05902 136.307   132.44078]\n",
      "  [130.39308 135.70848 132.00036]\n",
      "  [130.174   135.4736  131.89246]]\n",
      "\n",
      " [[130.0993  135.29484 131.36412]\n",
      "  [129.3446  134.45914 130.4656 ]\n",
      "  [130.2169  135.1767  131.10234]\n",
      "  [130.5724  135.43064 131.19912]\n",
      "  [131.15422 135.91802 131.53454]\n",
      "  [131.82228 136.48652 132.02346]\n",
      "  [132.36014 136.8891  132.37616]\n",
      "  [132.84044 137.28206 132.71158]\n",
      "  [133.39234 137.73926 133.10026]\n",
      "  [133.87552 138.1193  133.39128]\n",
      "  [134.26446 138.48954 133.70088]\n",
      "  [134.41534 138.60656 133.82908]\n",
      "  [134.63872 138.81284 134.00036]\n",
      "  [134.66304 138.81784 134.01392]\n",
      "  [134.64058 138.75158 133.97006]\n",
      "  [134.45728 138.54604 133.77428]\n",
      "  [134.40656 138.50472 133.67942]\n",
      "  [134.48728 138.5408  133.70702]\n",
      "  [134.46736 138.51216 133.68686]\n",
      "  [134.39664 138.48534 133.67322]\n",
      "  [134.15454 138.28714 133.46338]\n",
      "  [134.07836 138.28816 133.48564]\n",
      "  [133.88824 138.16576 133.39596]\n",
      "  [133.55484 137.92966 133.19618]\n",
      "  [132.96736 137.47828 132.81132]\n",
      "  [132.53954 137.1335  132.50728]\n",
      "  [131.9404  136.60228 132.01576]\n",
      "  [131.19426 135.97908 131.55256]\n",
      "  [130.54138 135.49654 131.19636]\n",
      "  [130.04254 135.13862 130.90018]\n",
      "  [129.33038 134.55898 130.49918]\n",
      "  [129.1568  134.38368 130.45104]]\n",
      "\n",
      " [[129.72472 134.64818 130.2514 ]\n",
      "  [128.71662 133.49748 129.01922]\n",
      "  [129.47348 134.05546 129.47586]\n",
      "  [129.85284 134.27762 129.55486]\n",
      "  [130.36156 134.59852 129.7368 ]\n",
      "  [130.96056 135.0099  130.0738 ]\n",
      "  [131.4814  135.3953  130.40452]\n",
      "  [132.00176 135.78396 130.7659 ]\n",
      "  [132.65174 136.30508 131.19688]\n",
      "  [133.13256 136.67622 131.49464]\n",
      "  [133.54978 137.03818 131.82024]\n",
      "  [133.77542 137.2144  132.00886]\n",
      "  [133.85434 137.29212 132.0447 ]\n",
      "  [133.87896 137.314   132.0798 ]\n",
      "  [133.79104 137.1742  131.99084]\n",
      "  [133.6203  136.94646 131.79082]\n",
      "  [133.66664 136.9696  131.76684]\n",
      "  [133.74678 136.99728 131.76378]\n",
      "  [133.6819  136.92362 131.68698]\n",
      "  [133.49932 136.8206  131.60342]\n",
      "  [133.25704 136.64758 131.45028]\n",
      "  [133.155   136.64464 131.42448]\n",
      "  [133.00288 136.59192 131.34896]\n",
      "  [132.62196 136.32228 131.09854]\n",
      "  [132.03974 135.90604 130.74132]\n",
      "  [131.56206 135.53186 130.43008]\n",
      "  [131.02044 135.09866 130.06374]\n",
      "  [130.42318 134.6961  129.78008]\n",
      "  [129.83214 134.34128 129.56334]\n",
      "  [129.21066 133.95824 129.29398]\n",
      "  [128.59094 133.54152 129.06528]\n",
      "  [128.59134 133.55772 129.22356]]\n",
      "\n",
      " [[129.46194 134.08636 129.25308]\n",
      "  [128.37236 132.81596 127.87732]\n",
      "  [129.05286 133.21986 128.14596]\n",
      "  [129.50614 133.43788 128.22288]\n",
      "  [129.87256 133.54682 128.1991 ]\n",
      "  [130.3155  133.7125  128.29632]\n",
      "  [130.81566 133.97836 128.5313 ]\n",
      "  [131.44898 134.4302  128.95624]\n",
      "  [132.16334 135.02226 129.46408]\n",
      "  [132.66618 135.40826 129.78268]\n",
      "  [133.00956 135.66434 129.99422]\n",
      "  [133.20926 135.81182 130.17048]\n",
      "  [133.14612 135.7364  130.0978 ]\n",
      "  [133.22544 135.7934  130.15598]\n",
      "  [133.22832 135.74978 130.15812]\n",
      "  [133.03212 135.46578 129.93234]\n",
      "  [133.05078 135.43424 129.84192]\n",
      "  [133.10594 135.44444 129.8232 ]\n",
      "  [132.98768 135.33892 129.72028]\n",
      "  [132.82542 135.26762 129.6637 ]\n",
      "  [132.53074 135.05518 129.4546 ]\n",
      "  [132.318   134.9589  129.33782]\n",
      "  [132.23588 134.99426 129.29862]\n",
      "  [131.88462 134.78526 129.09698]\n",
      "  [131.37134 134.47092 128.82706]\n",
      "  [130.90194 134.18106 128.59948]\n",
      "  [130.36776 133.8307  128.33834]\n",
      "  [129.92408 133.62768 128.22776]\n",
      "  [129.4523  133.43796 128.17744]\n",
      "  [128.83038 133.1329  128.00818]\n",
      "  [128.2162  132.7837  127.84486]\n",
      "  [128.31966 132.95378 128.17792]]\n",
      "\n",
      " [[129.20144 133.52226 128.26302]\n",
      "  [128.05928 132.14512 126.7566 ]\n",
      "  [128.62808 132.38562 126.84076]\n",
      "  [129.05242 132.47504 126.80482]\n",
      "  [129.27532 132.35908 126.5877 ]\n",
      "  [129.72544 132.47142 126.67834]\n",
      "  [130.23076 132.6474  126.82508]\n",
      "  [130.78548 132.95108 127.08904]\n",
      "  [131.41694 133.3858  127.46544]\n",
      "  [131.82248 133.65932 127.6672 ]\n",
      "  [132.27446 134.02502 127.96092]\n",
      "  [132.53312 134.22148 128.16602]\n",
      "  [132.42424 134.07706 128.0388 ]\n",
      "  [132.45246 134.07574 128.06056]\n",
      "  [132.48814 134.0788  128.11722]\n",
      "  [132.28718 133.7913  127.88048]\n",
      "  [132.24086 133.69914 127.75788]\n",
      "  [132.31714 133.75866 127.81614]\n",
      "  [132.31526 133.78912 127.8597 ]\n",
      "  [132.0881  133.63864 127.70492]\n",
      "  [131.66718 133.30748 127.33888]\n",
      "  [131.3998  133.14576 127.14958]\n",
      "  [131.25342 133.15732 127.11574]\n",
      "  [130.97854 133.08794 127.0425 ]\n",
      "  [130.61822 132.94174 126.93126]\n",
      "  [130.25482 132.82794 126.849  ]\n",
      "  [129.77834 132.62916 126.71084]\n",
      "  [129.33994 132.49012 126.63622]\n",
      "  [128.9286  132.39206 126.67178]\n",
      "  [128.4043  132.23054 126.63976]\n",
      "  [127.90974 132.07546 126.64712]\n",
      "  [128.04658 132.36106 127.11556]]\n",
      "\n",
      " [[128.90614 132.91964 127.25744]\n",
      "  [127.7297  131.45066 125.64824]\n",
      "  [128.11824 131.42796 125.46508]\n",
      "  [128.43866 131.34756 125.27106]\n",
      "  [128.61648 131.12106 124.96624]\n",
      "  [129.03524 131.14768 124.99014]\n",
      "  [129.47552 131.20126 125.00872]\n",
      "  [129.87758 131.24268 125.00232]\n",
      "  [130.4613  131.5412  125.27804]\n",
      "  [130.92934 131.80738 125.47934]\n",
      "  [131.4128  132.17688 125.777  ]\n",
      "  [131.70706 132.3989  125.9809 ]\n",
      "  [131.5502  132.1617  125.74036]\n",
      "  [131.58314 132.14484 125.77628]\n",
      "  [131.62334 132.15632 125.85594]\n",
      "  [131.44156 131.9128  125.6617 ]\n",
      "  [131.3679  131.83732 125.54312]\n",
      "  [131.51588 132.00896 125.71604]\n",
      "  [131.59592 132.1077  125.84338]\n",
      "  [131.3867  131.9394  125.65846]\n",
      "  [131.0738  131.74254 125.41646]\n",
      "  [130.79064 131.60976 125.23832]\n",
      "  [130.5732  131.63176 125.25398]\n",
      "  [130.21772 131.5338  125.19678]\n",
      "  [129.95054 131.5113  125.1872 ]\n",
      "  [129.75014 131.62946 125.2976 ]\n",
      "  [129.29718 131.53532 125.23692]\n",
      "  [128.80762 131.43408 125.15088]\n",
      "  [128.42388 131.41012 125.23634]\n",
      "  [127.956   131.35474 125.2919 ]\n",
      "  [127.52876 131.32052 125.42292]\n",
      "  [127.7188  131.70992 125.98894]]\n",
      "\n",
      " [[128.48642 132.18622 126.05574]\n",
      "  [127.2023  130.59252 124.32386]\n",
      "  [127.51932 130.40954 124.02192]\n",
      "  [127.70708 130.12214 123.63592]\n",
      "  [127.95318 129.91436 123.33798]\n",
      "  [128.27146 129.78012 123.17794]\n",
      "  [128.68646 129.69774 123.10868]\n",
      "  [129.04666 129.62316 123.02766]\n",
      "  [129.60044 129.81418 123.20156]\n",
      "  [130.15034 130.01428 123.39064]\n",
      "  [130.60054 130.2255  123.56628]\n",
      "  [130.80266 130.33894 123.64974]\n",
      "  [130.64374 130.13266 123.41286]\n",
      "  [130.6547  130.06124 123.39274]\n",
      "  [130.7336  130.1019  123.5027 ]\n",
      "  [130.63078 129.97634 123.43652]\n",
      "  [130.51892 129.92162 123.34116]\n",
      "  [130.64934 130.09338 123.50626]\n",
      "  [130.73216 130.19742 123.62222]\n",
      "  [130.57618 130.10418 123.5062 ]\n",
      "  [130.39298 130.06804 123.41748]\n",
      "  [130.1014  129.94552 123.21546]\n",
      "  [129.77698 129.89978 123.15474]\n",
      "  [129.47482 129.9307  123.26272]\n",
      "  [129.226   130.00354 123.34036]\n",
      "  [129.06916 130.2618  123.57906]\n",
      "  [128.71392 130.34034 123.66992]\n",
      "  [128.22466 130.28992 123.6459 ]\n",
      "  [127.8525  130.37286 123.77714]\n",
      "  [127.41782 130.42932 123.90016]\n",
      "  [127.03252 130.45788 124.12034]\n",
      "  [127.31718 130.96872 124.79866]]\n",
      "\n",
      " [[127.99512 131.40144 124.77998]\n",
      "  [126.59798 129.64    122.92084]\n",
      "  [126.86588 129.34952 122.5265 ]\n",
      "  [126.99254 128.95166 122.02038]\n",
      "  [127.15416 128.6231  121.60828]\n",
      "  [127.5778  128.50478 121.47362]\n",
      "  [127.96546 128.30938 121.34888]\n",
      "  [128.15948 128.01094 121.12098]\n",
      "  [128.64642 128.0301  121.14062]\n",
      "  [129.26456 128.1982  121.29606]\n",
      "  [129.68334 128.2906  121.36652]\n",
      "  [129.7618  128.23214 121.3107 ]\n",
      "  [129.61996 128.01328 121.04682]\n",
      "  [129.73826 128.01416 121.09376]\n",
      "  [129.84806 128.05174 121.21926]\n",
      "  [129.75678 127.94564 121.16368]\n",
      "  [129.7989  128.06602 121.26842]\n",
      "  [129.98784 128.28426 121.44514]\n",
      "  [130.0171  128.36696 121.49296]\n",
      "  [129.79592 128.26042 121.37678]\n",
      "  [129.557   128.20468 121.27282]\n",
      "  [129.39826 128.2232  121.19718]\n",
      "  [129.09896 128.24062 121.1969 ]\n",
      "  [128.89804 128.43454 121.43016]\n",
      "  [128.53148 128.5     121.49988]\n",
      "  [128.29888 128.7516  121.74984]\n",
      "  [128.08498 129.05076 122.03758]\n",
      "  [127.66116 129.12702 122.1287 ]\n",
      "  [127.28934 129.26746 122.2781 ]\n",
      "  [126.82912 129.37896 122.41428]\n",
      "  [126.53384 129.5361  122.75042]\n",
      "  [126.8835  130.16582 123.52832]]\n",
      "\n",
      " [[127.45488 130.53434 123.46928]\n",
      "  [125.96096 128.6187  121.45428]\n",
      "  [126.22518 128.29164 121.03006]\n",
      "  [126.29102 127.8015  120.4674 ]\n",
      "  [126.45806 127.44894 120.04748]\n",
      "  [126.88426 127.256   119.82316]\n",
      "  [127.24388 126.97522 119.61014]\n",
      "  [127.30866 126.47394 119.20674]\n",
      "  [127.71134 126.30814 119.09348]\n",
      "  [128.32756 126.4432  119.2027 ]\n",
      "  [128.70442 126.42796 119.16396]\n",
      "  [128.87336 126.39192 119.19274]\n",
      "  [128.93242 126.27988 119.0879 ]\n",
      "  [129.10684 126.25116 119.12086]\n",
      "  [129.25286 126.27972 119.21704]\n",
      "  [129.3737  126.378   119.33604]\n",
      "  [129.5096  126.58832 119.55084]\n",
      "  [129.60204 126.75692 119.70606]\n",
      "  [129.59876 126.86692 119.76288]\n",
      "  [129.42442 126.84826 119.71014]\n",
      "  [129.01668 126.66094 119.5022 ]\n",
      "  [128.7837  126.651   119.44946]\n",
      "  [128.6435  126.87354 119.61558]\n",
      "  [128.49028 127.1366  119.84044]\n",
      "  [128.17928 127.32562 119.98778]\n",
      "  [127.89364 127.60528 120.25848]\n",
      "  [127.58724 127.89012 120.50522]\n",
      "  [127.23842 128.11868 120.7386 ]\n",
      "  [126.76966 128.17478 120.7836 ]\n",
      "  [126.27504 128.3042  120.92644]\n",
      "  [125.97282 128.55654 121.3336 ]\n",
      "  [126.4166  129.33416 122.23266]]\n",
      "\n",
      " [[127.22844 129.92706 122.42308]\n",
      "  [125.68746 127.94512 120.30852]\n",
      "  [125.81298 127.47388 119.76716]\n",
      "  [125.85342 126.90016 119.15446]\n",
      "  [126.068   126.5498  118.7329 ]\n",
      "  [126.41196 126.23094 118.38984]\n",
      "  [126.8012  125.91292 118.14306]\n",
      "  [126.85884 125.27852 117.58998]\n",
      "  [127.21956 124.98364 117.37084]\n",
      "  [127.75622 124.9722  117.38032]\n",
      "  [128.1443  124.91906 117.31304]\n",
      "  [128.46076 124.94468 117.44466]\n",
      "  [128.74824 124.95918 117.52374]\n",
      "  [128.94578 124.8874  117.52588]\n",
      "  [129.07996 124.84332 117.54042]\n",
      "  [129.19524 124.9111  117.64586]\n",
      "  [129.36068 125.17706 117.92262]\n",
      "  [129.3598  125.3049  118.07452]\n",
      "  [129.45744 125.53722 118.27526]\n",
      "  [129.36636 125.61576 118.26738]\n",
      "  [128.9068  125.46946 118.0265 ]\n",
      "  [128.5037  125.41428 117.90988]\n",
      "  [128.1774  125.50178 117.94632]\n",
      "  [128.01422 125.80468 118.18458]\n",
      "  [127.82222 126.2303  118.53594]\n",
      "  [127.63718 126.6687  118.92462]\n",
      "  [127.27772 126.94152 119.1286 ]\n",
      "  [126.92268 127.20776 119.39566]\n",
      "  [126.38504 127.23238 119.42434]\n",
      "  [125.86472 127.41692 119.64022]\n",
      "  [125.562   127.769   120.09916]\n",
      "  [126.12176 128.68086 121.14128]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m dev_data \u001b[38;5;241m=\u001b[39m dev_data \u001b[38;5;241m-\u001b[39m mean_img\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 3: Append the bias dimension of ones (bias trick)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m train_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([train_data, np\u001b[38;5;241m.\u001b[39mones((train_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))])\n\u001b[0;32m     34\u001b[0m val_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([val_data, np\u001b[38;5;241m.\u001b[39mones((val_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))])\n\u001b[0;32m     35\u001b[0m test_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([test_data, np\u001b[38;5;241m.\u001b[39mones((test_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jasha\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Split test_data into validation and test sets\n",
    "val_data = test_data[:5000]\n",
    "test_data = test_data[5000:]\n",
    "\n",
    "# Create a small development set\n",
    "dev_data = train_data[:1000]  # Subset of train_data for development purposes\n",
    "\n",
    "# Preprocessing: subtract the mean image\n",
    "# 1: Create the mean image based on training data\n",
    "mean_img = np.mean(train_data, axis=0)\n",
    "print(\"Mean image (first 10 elements):\", mean_img[:10])  # print a few of the elements\n",
    "\n",
    "# Visualize the mean image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mean_img.reshape((32, 32, 3)).astype('uint8'))  # visualize the mean image\n",
    "plt.title('Mean Image')\n",
    "plt.show()\n",
    "\n",
    "# 2: Subtract the mean image from training, validation, test, and development data\n",
    "train_data = train_data - mean_img\n",
    "val_data = val_data - mean_img\n",
    "test_data = test_data - mean_img\n",
    "dev_data = dev_data - mean_img\n",
    "\n",
    "# 3: Append the bias dimension of ones (bias trick)\n",
    "train_data = np.hstack([train_data, np.ones((train_data.shape[0], 1))])\n",
    "val_data = np.hstack([val_data, np.ones((val_data.shape[0], 1))])\n",
    "test_data = np.hstack([test_data, np.ones((test_data.shape[0], 1))])\n",
    "dev_data = np.hstack([dev_data, np.ones((dev_data.shape[0], 1))])\n",
    "\n",
    "# Print the shapes of the processed datasets\n",
    "print(\"Shapes after processing:\")\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "print(\"Development data shape:\", dev_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier\n",
    "\n",
    "You need to complete `svm_loss_naive` which uses for loops to evaluate the multiclass SVM loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from past.builtins import xrange\n",
    "\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, naive implementation (with loops).\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    # compute the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1 # note delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "\n",
    "    # Right now the loss is a sum over all training examples, but we want it\n",
    "    # to be an average instead so we divide by num_train.\n",
    "    loss /= num_train\n",
    "\n",
    "    # Add regularization to the loss.\n",
    "    loss += reg * np.sum(W * W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Compute the gradient of the loss function and store it dW.                #\n",
    "    # Rather than first computing the loss and then computing the derivative,   #\n",
    "    # it may be simpler to compute the derivative at the same time that the     #\n",
    "    # loss is being computed. As a result you may need to modify some of the    #\n",
    "    # code above to compute the gradient.                                       #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.425408\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "\n",
    "# Flatten X_dev from (32, 32, 3) to (num_samples, 3072) and append bias trick\n",
    "X_dev = X_dev.reshape(X_dev.shape[0], -1)\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])  # Shape should be (num_samples, 3073)\n",
    "\n",
    "# Assume y_dev is already loaded correctly and has the correct shape (num_samples,)\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "\n",
    "print('loss: %f' % (loss,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `svm_loss_naive`. You will find it helpful to interleave your new code inside the existing function.\n",
    "\n",
    "To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradient_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradient_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grad_check_sparse\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvm_loss_naive\u001b[39m(W, X, y, reg):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Structured SVM loss function, naive implementation (with loops).\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    - gradient with respect to weights W; an array of same shape as W\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradient_check'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gradient_check import grad_check_sparse\n",
    "\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, naive implementation (with loops).\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1  # note delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:, j] += X[i]\n",
    "                dW[:, y[i]] -= X[i]\n",
    "\n",
    "    # Average loss and gradient\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Add regularization to the loss and gradient\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "# Initialize variables\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 0.0001  # Example weight matrix (D, C)\n",
    "X_dev = np.random.randn(50, 3073)       # Example data (N, D)\n",
    "y_dev = np.random.randint(10, size=50)  # Example labels (N,)\n",
    "\n",
    "# Compute loss and gradient without regularization\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Define a function that returns loss for gradient checking\n",
    "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "\n",
    "# Perform gradient check without regularization\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# Compute loss and gradient with regularization\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "\n",
    "# Define a function that returns loss for gradient checking with regularization\n",
    "f = lambda w: svm_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "\n",
    "# Perform gradient check with regularization\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradient_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/directory/containing/gradient_check\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradient_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grad_check_sparse\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradient_check'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/directory/containing/gradient_check')\n",
    "from gradient_check import grad_check_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with regularization: 9.018263694641654\n",
      "Gradient with regularization (first 5 rows, 5 columns):\n",
      "[[ 0.45896767  0.51263295 -0.45695758  0.34012832  0.37513293]\n",
      " [ 0.84239999  0.00633881  0.01997765  0.35947004 -0.13964541]\n",
      " [-0.2158925   0.47502084 -0.31442936  0.0087179   0.47925636]\n",
      " [ 0.10311758 -0.11379617 -0.62758659  0.17152457 -0.18042518]\n",
      " [ 0.11492964 -0.53192143  0.66701926  0.38379621 -0.56721553]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function with regularization, naive implementation (with loops).\n",
    "    \n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: A single float representing the loss\n",
    "    - dW: The gradient with respect to weights W, same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)  # Initialize gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    \n",
    "    # Compute the loss and the gradient\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1  # Delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:, j] += X[i]  # Accumulate gradient for incorrect class\n",
    "                dW[:, y[i]] -= X[i]  # Accumulate gradient for correct class\n",
    "    \n",
    "    # Average the loss and gradient\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Regularization: Add regularization to the loss\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    # Regularization: Add gradient of regularization term\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "# Example test case to verify the loss with regularization\n",
    "\n",
    "# Initialize random weights and data\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 0.0001  # Example weight matrix (D, C)\n",
    "X_dev = np.random.randn(50, 3073)       # Example data (N, D)\n",
    "y_dev = np.random.randint(10, size=50)  # Example labels (N,)\n",
    "\n",
    "# Compute loss and gradient with regularization\n",
    "reg_strength = 5e1  # Example regularization strength (lambda)\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, reg_strength)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Loss with regularization: {loss}\")\n",
    "print(f\"Gradient with regularization (first 5 rows, 5 columns):\\n{grad[:5, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss without regularization: [9.3386983]\n",
      "\n",
      "Performing gradient check...\n",
      "\n",
      "Index (1240, 4): numerical: [-19.014], analytic: 0.0, relative error: [1.]\n",
      "Index (509, 0): numerical: [-83.288], analytic: 0.0, relative error: [1.]\n",
      "Index (733, 9): numerical: [50.48430516], analytic: 0.0, relative error: [1.]\n",
      "Index (371, 2): numerical: [94.968], analytic: 0.0, relative error: [1.]\n",
      "Index (347, 0): numerical: [-86.182], analytic: 0.0, relative error: [1.]\n",
      "Index (504, 4): numerical: [0.918], analytic: 0.0, relative error: [1.]\n",
      "Index (3040, 1): numerical: [-6.61943468], analytic: 0.0, relative error: [1.]\n",
      "Index (894, 7): numerical: [6.55212952], analytic: 0.0, relative error: [1.]\n",
      "Index (1068, 2): numerical: [90.768], analytic: 0.0, relative error: [1.]\n",
      "Index (937, 7): numerical: [5.068], analytic: 0.0, relative error: [1.]\n",
      "Loss with regularization: [9.35395398]\n",
      "\n",
      "Performing gradient check...\n",
      "\n",
      "Index (2096, 8): numerical: [-15.42681729], analytic: 0.0, relative error: [1.]\n",
      "Index (1849, 2): numerical: [72.9289538], analytic: 0.0, relative error: [1.]\n",
      "Index (2100, 9): numerical: [59.16832317], analytic: 0.0, relative error: [1.]\n",
      "Index (1002, 1): numerical: [6.42353244], analytic: 0.0, relative error: [1.]\n",
      "Index (392, 7): numerical: [-9.11998721], analytic: 0.0, relative error: [1.]\n",
      "Index (71, 8): numerical: [-40.92193872], analytic: 0.0, relative error: [1.]\n",
      "Index (1468, 6): numerical: [-40.28739002], analytic: 0.0, relative error: [1.]\n",
      "Index (1859, 7): numerical: [14.36048361], analytic: 0.0, relative error: [1.]\n",
      "Index (146, 5): numerical: [44.14710466], analytic: 0.0, relative error: [1.]\n",
      "Index (549, 2): numerical: [94.33353597], analytic: 0.0, relative error: [1.]\n",
      "Gradient @done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  Initialize random weights for W (small random values)\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "\n",
    "#  Compute the loss and the gradient using the naive SVM loss function, without regularization\n",
    "\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Print\n",
    "print(f\"Initial loss without regularization: {loss}\")\n",
    "\n",
    "def loss_function_no_reg(w):\n",
    "    return svm_loss_naive(w, X_dev, y_dev, 0.0)[0]  # We only need the loss (first element of the tuple)\n",
    "\n",
    "# gradient check function\n",
    "\n",
    "def grad_check_sparse(f, W, grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    This function checks the gradient by comparing the numerical approximation with the analytical gradient.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: the function that computes the loss (should return only the loss)\n",
    "    - W: the current weights\n",
    "    - grad: the analytical gradient that we computed\n",
    "    - num_checks: the number of random gradient components to check (default is 10)\n",
    "    - h: the small step size for numerical gradient approximation (default is 1e-5)\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming gradient check...\\n\")\n",
    "    \n",
    "    # Loop over the number of gradient checks \n",
    "    for i in range(num_checks):\n",
    "        # Randomly select an index within the shape of W (tuple of coordinates)\n",
    "        ix = tuple([np.random.randint(m) for m in W.shape])\n",
    "        \n",
    "        # Save the original value at the randomly selected index\n",
    "        oldval = W[ix]\n",
    "        \n",
    "        W[ix] = oldval + h\n",
    "        fxph = f(W)  # Loss for W + h\n",
    "        \n",
    "        \n",
    "        W[ix] = oldval - h\n",
    "        fxmh = f(W)  # Loss for W - h\n",
    "        W[ix] = oldval\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = grad[ix]\n",
    "        \n",
    "        # relative error between the numerical and analytical gradients\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / max(1e-8, abs(grad_numerical) + abs(grad_analytic))\n",
    "        \n",
    "        # Print the numerical gradient, analytical gradient, and relative error for the selected index\n",
    "        print(f\"Index {ix}: numerical: {grad_numerical}, analytic: {grad_analytic}, relative error: {rel_error}\")\n",
    "\n",
    "#Perform the gradient check without regularization\n",
    "grad_check_sparse(loss_function_no_reg, W, grad)\n",
    "\n",
    "loss_reg, grad_reg = svm_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "\n",
    "# Print the loss with regularization\n",
    "print(f\"Loss with regularization: {loss_reg}\")\n",
    "\n",
    "def loss_function_with_reg(w):\n",
    "    return svm_loss_naive(w, X_dev, y_dev, 5e1)[0]  # We only need the loss (first element of the tuple)\n",
    "\n",
    "# Step 8: Perform the gradient check with regularization\n",
    "grad_check_sparse(loss_function_with_reg, W, grad_reg)\n",
    "\n",
    "# Step 9: End of the gradient check\n",
    "print(\"Gradient @done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "    Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement a vectorized version of the structured SVM loss, storing the    #\n",
    "    # result in loss.                                                           #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement a vectorized version of the gradient for the structured SVM     #\n",
    "    # loss, storing the result in dW.                                           #\n",
    "    #                                                                           #\n",
    "    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n",
    "    # to reuse some of the intermediate values that you used to compute the     #\n",
    "    # loss.                                                                     #\n",
    "    #############################################################################\n",
    "    # *****START OF YOUR CODE*****\n",
    "\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE*****\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    - loss: (float) the SVM loss\n",
    "    - dW: (numpy array) gradient of the loss with respect to W, of the same shape as W\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    dW = np.zeros(W.shape)  # Initialize the loss and gradient\n",
    "\n",
    "   Compute the scores for all classes\n",
    "    scores = X.dot(W)  \n",
    " \n",
    "    # Step 2: Get the correct class scores (for each row, select the score corresponding to the correct class)\n",
    "    correct_class_scores = scores[np.arange(X.shape[0]), y].reshape(-1, 1)  # Shape (N, 1)\n",
    "\n",
    "    # Step 3: Calculate the margins\n",
    "    margins = np.maximum(0, scores - correct_class_scores + 1)  # Shape (N, C)\n",
    "    margins[np.arange(X.shape[0]), y] = 0  # No loss for the correct class\n",
    "\n",
    "    # Step 4: Compute the loss\n",
    "    loss = np.sum(margins) / X.shape[0]  # Take the mean of the sum of margins\n",
    "    loss += reg * np.sum(W * W)  # Add regularization term to the loss\n",
    "\n",
    "    # Step 5: Compute the gradient\n",
    "    binary = (margins > 0).astype(float)  # Binary mask of where the margins are positive\n",
    "    row_sum = np.sum(binary, axis=1)  # Sum of positive margins for each data point\n",
    "    binary[np.arange(X.shape[0]), y] = -row_sum  # Adjust for the correct classes\n",
    "\n",
    "    # Compute the gradient using the binary mask\n",
    "    dW = X.T.dot(binary) / X.shape[0]  # Calculate gradient\n",
    "    dW += 2 * reg * W  # Add the regularization gradient\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 14.766031970988527\n",
      "Gradient dW:\n",
      " [[-6.26679814e-01 -3.12578892e-01  1.46833398e+00  9.59010514e-01\n",
      "   2.19214739e-01 -1.04481756e+00]\n",
      " [-6.66906497e-01  1.99347681e-01  1.61948057e-01  2.71835806e-01\n",
      "  -2.92888057e-01  7.24239260e-01]\n",
      " [-7.12856334e-01  1.96425383e-02  4.11246865e-01  1.11803883e+00\n",
      "  -1.57811810e-01 -6.17785674e-04]\n",
      " [ 3.23030493e-01 -3.52545253e-01 -5.63601303e-01  1.90465780e-01\n",
      "  -2.35292036e-01  8.38902544e-01]\n",
      " [-9.75713213e-02 -5.23116502e-01  3.41804486e-01  1.07429734e+00\n",
      "   5.31492223e-01 -3.23708895e-01]\n",
      " [ 9.02759826e-02  2.93356316e-01  2.31271824e-01  4.77090809e-01\n",
      "   3.02605721e-01 -9.19493819e-01]\n",
      " [-2.82231446e-01  1.86752103e-01  6.70482222e-02  5.47755639e-01\n",
      "  -4.77231554e-02  1.59856076e-02]\n",
      " [-7.16039321e-02 -2.33938483e-01 -3.27098382e-01  1.10017651e+00\n",
      "   1.22498551e-02 -1.49852690e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    - loss: (float) the SVM loss\n",
    "    - dW: (numpy array) gradient of the loss with respect to W, of the same shape as W\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape)   # Initialize the loss and gradient\n",
    "\n",
    "    # Step 1: Compute the scores for all classes\n",
    "    scores = X.dot(W)  # Shape (N, C) - Scores for all classes for all data points\n",
    "\n",
    "    # Step 2: Get the correct class scores (for each row, select the score corresponding to the correct class)\n",
    "    correct_class_scores = scores[np.arange(X.shape[0]), y].reshape(-1, 1)  # Shape (N, 1)\n",
    "\n",
    "    # Step 3: Calculate the margins\n",
    "    margins = np.maximum(0, scores - correct_class_scores + 1)  \n",
    "    margins[np.arange(X.shape[0]), y] = 0  #  Calculate the margins\n",
    "\n",
    "    \n",
    "    loss = np.sum(margins) / X.shape[0]  \n",
    "    loss += reg * np.sum(W * W)  \n",
    "\n",
    "    \n",
    "    binary = (margins > 0).astype(float)  \n",
    "    row_sum = np.sum(binary, axis=1)  \n",
    "    binary[np.arange(X.shape[0]), y] = -row_sum  \n",
    "\n",
    "    # Compute the gradient using the binary mask\n",
    "    dW = X.T.dot(binary) / X.shape[0]  # Calculate gradient\n",
    "    dW += 2 * reg * W  # Add the regularization gradient\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "# Generate dummy data for testing\n",
    "np.random.seed(0)  # For reproducibility\n",
    "N = 10\n",
    "D = 8 \n",
    "C = 6 \n",
    "\n",
    "# Randomly generate some data\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)  # Random labels between 0 and C-1\n",
    "W = np.random.randn(D, C)  # Random weight matrix\n",
    "reg = 0.1  # Regularization strength\n",
    "\n",
    "# Call the SVM loss function\n",
    "loss, dW = svm_loss_vectorized(W, X, y, reg)\n",
    "\n",
    "# Print the results\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Gradient dW:\\n\", dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 0.032753s\n",
      "Vectorized loss and gradient: computed in 0.072947s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# The naive implementation and the vectorized implementation should match, but\n",
    "# the vectorized version should still be much faster.\n",
    "import time\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is create with the formulation method #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 0.032091s\n",
      "Vectorized loss and gradient: computed in 0.084646s\n",
      "Gradient difference (Frobenius norm): 0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def svm_loss_naive(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, naive implementation (with loops).\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape)  # Initialize gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            margin = scores[j] - correct_class_score + 1  # Delta = 1\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:, j] += X[i]\n",
    "                dW[:, y[i]] -= X[i]\n",
    "    \n",
    "    # Average loss and gradient\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "    # Regularization\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, vectorized implementation.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = X.dot(W)\n",
    "    \n",
    "    # Select correct class scores (N,)\n",
    "    correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1)\n",
    "    \n",
    "    # Compute margins\n",
    "    margins = np.maximum(0, scores - correct_class_scores + 1)  # delta = 1\n",
    "    margins[np.arange(num_train), y] = 0  # Do not consider correct class scores\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = np.sum(margins) / num_train\n",
    "    loss += 0.5 * reg * np.sum(W * W)  # Regularization\n",
    "\n",
    "    # Compute gradient\n",
    "    margins[margins > 0] = 1\n",
    "    row_sum = np.sum(margins, axis=1)  # Count how many times each row had a positive margin\n",
    "    margins[np.arange(num_train), y] -= row_sum\n",
    "    dW = X.T.dot(margins) / num_train\n",
    "    \n",
    "    # Regularization gradient\n",
    "    dW += reg * W\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "# Initialize variables\n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3073, 10) * 0.0001  # Weight matrix (D, C)\n",
    "X_dev = np.random.randn(50, 3073)       # Data matrix (N, D)\n",
    "y_dev = np.random.randint(10, size=50)  # Labels (N,)\n",
    "\n",
    "# Compare naive and vectorized implementations\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# Compare the gradients using Frobenius norm\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Gradient difference (Frobenius norm): %f' % difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that your SVM implementation is working correctly and that the vectorized approach is behaving as intended, potentially offering better performance with larger datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
