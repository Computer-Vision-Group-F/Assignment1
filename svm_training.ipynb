{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-title"
        ],
        "id": "3rKz9ktvHl5P"
      },
      "source": [
        "# Assignment 1-2: Support Vector Machine\n",
        "\n",
        "## Multiclass Support Vector Machine exercise\n",
        "\n",
        "\n",
        "In this exercise you will:\n",
        "    \n",
        "- implement a fully-vectorized **loss function** for the SVM\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** using numerical gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "Uf6y7OOHHl5V"
      },
      "source": [
        "## CIFAR-10 Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "51CMqvqQHl5X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import data_utils\n",
        "import download\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "download_dir = \"./data\"\n",
        "download.maybe_download_and_extract(url,download_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvKNulp2IOSN",
        "outputId": "c0d9b379-f667-48fc-df10-cf1a1e8f2d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_dir = './data/cifar-10-batches-py'\n",
        "X_train, y_train, X_test, y_test = data_utils.load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# Checking the size of the training and testing data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pd6ASbiInEa",
        "outputId": "543c19db-01c4-4f55-9e22-6b4b6b02e950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (50000, 32, 32, 3)\n",
            "Training labels shape:  (50000,)\n",
            "Test data shape:  (10000, 32, 32, 3)\n",
            "Test labels shape:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkBn6jRdHl5Z",
        "outputId": "bd4060a9-af69-4380-e0f5-cd1af3c32741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (49000, 32, 32, 3)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 32, 32, 3)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 32, 32, 3)\n",
            "Test labels shape:  (1000,)\n"
          ]
        }
      ],
      "source": [
        "# Split the data into train, val, and test sets. In addition we will\n",
        "# create a small development set as a subset of the training data;\n",
        "# we can use this for development so our code runs faster.\n",
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "num_dev = 500\n",
        "\n",
        "# Our validation set will be num_validation points from the original\n",
        "# training set.\n",
        "mask = range(num_training, num_training + num_validation)\n",
        "X_val = X_train[mask]\n",
        "y_val = y_train[mask]\n",
        "\n",
        "# Our training set will be the first num_train points from the original\n",
        "# training set.\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "# We will also make a development set, which is a small subset of\n",
        "# the training set.\n",
        "mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "X_dev = X_train[mask]\n",
        "y_dev = y_train[mask]\n",
        "\n",
        "# We use the first num_test points of the original test set as our\n",
        "# test set.\n",
        "mask = range(num_test)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tGDOAffHl5a",
        "outputId": "1974ae24-92b2-491b-f684-10e0019019fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape:  (49000, 3072)\n",
            "Validation data shape:  (1000, 3072)\n",
            "Test data shape:  (1000, 3072)\n",
            "dev data shape:  (500, 3072)\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing: reshape the image data into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "\n",
        "# As a sanity check, print out the shapes of the data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "BbSTQo2tHl5a",
        "outputId": "19f0a6b6-4ce0-4413-8826-3eda766b2484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130.64189796 135.98173469 132.47391837 130.05569388 135.34804082\n",
            " 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) # print a few of the elements\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train -= mean_image\n",
        "X_val -= mean_image\n",
        "X_test -= mean_image\n",
        "X_dev -= mean_image\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iX9pam9Hl5b"
      },
      "source": [
        "## SVM Classifier\n",
        "\n",
        "You need to complete `svm_loss_naive` which uses for loops to evaluate the multiclass SVM loss function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from builtins import range\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from past.builtins import xrange\n",
        "\n",
        "def svm_loss_naive(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, naive implementation (with loops).\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    dW = np.zeros(W.shape)  # initialize the gradient as zero\n",
        "\n",
        "    # compute the loss and the gradient\n",
        "    num_classes = W.shape[1]\n",
        "    num_train = X.shape[0]\n",
        "    loss = 0.0\n",
        "    for i in range(num_train):\n",
        "        scores = X[i].dot(W)\n",
        "        correct_class_score = scores[y[i]]\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                continue\n",
        "            margin = scores[j] - correct_class_score + 1  # note delta = 1\n",
        "            if margin > 0:\n",
        "                loss += margin\n",
        "\n",
        "                # Compute gradient\n",
        "                dW[:, j] += X[i]      # For incorrect class\n",
        "                dW[:, y[i]] -= X[i]   # For correct class\n",
        "\n",
        "    # Average loss over number of training examples\n",
        "    loss /= num_train\n",
        "\n",
        "    # Average gradient over number of training examples\n",
        "    dW /= num_train\n",
        "\n",
        "    # Add regularization to the loss and the gradient\n",
        "    loss += reg * np.sum(W * W)\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n"
      ],
      "metadata": {
        "id": "soo3lIvxcgZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Calculation"
      ],
      "metadata": {
        "id": "SYAnK33TppEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsgEKy1fHl5b",
        "outputId": "01e26313-ff6f-4b64-d15d-ddf622c08437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 9.069487\n"
          ]
        }
      ],
      "source": [
        "# generate a random SVM weight matrix of small numbers\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "\n",
        "loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "print('loss: %f' % (loss, ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, vectorized implementation.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "         that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    N = X.shape[0]  # number of training examples\n",
        "    scores = X.dot(W)  # Compute the scores (N, C)\n",
        "    correct_class_scores = scores[np.arange(N), y].reshape(N, 1)  # Correct class scores\n",
        "\n",
        "    # Compute margins\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)  # (N, C)\n",
        "    margins[np.arange(N), y] = 0  # Zero out the correct class scores\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = np.sum(margins) / N\n",
        "    loss += 0.5 * reg * np.sum(W * W)  # Regularization term\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "chHWl-20Jvuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_gradient_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM gradient, vectorized implementation.\n",
        "\n",
        "    Inputs and outputs are the same as svm_loss_vectorized.\n",
        "    \"\"\"\n",
        "    N = X.shape[0]\n",
        "    scores = X.dot(W)\n",
        "    correct_class_scores = scores[np.arange(N), y].reshape(N, 1)\n",
        "\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)\n",
        "    margins[np.arange(N), y] = 0\n",
        "\n",
        "    # Binary mask for margins greater than 0\n",
        "    binary = margins\n",
        "    binary[margins > 0] = 1\n",
        "\n",
        "    row_sum = np.sum(binary, axis=1)  # Sum over classes\n",
        "    binary[np.arange(N), y] = -row_sum  # For correct classes\n",
        "\n",
        "    dW = X.T.dot(binary) / N  # Compute gradient\n",
        "    dW += reg * W  # Regularization gradient\n",
        "\n",
        "    return dW\n"
      ],
      "metadata": {
        "id": "Pc7XHZ_DJ-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(f, W, h=1e-5):\n",
        "    \"\"\"\n",
        "    A naive implementation of numerical gradient of f at W.\n",
        "    - f: a function that takes a single argument and outputs a loss\n",
        "    - W: the point (numpy array) to compute the gradient at\n",
        "    - h: step size\n",
        "\n",
        "    Returns:\n",
        "    - grad: a numpy array of same shape as W representing the gradient\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(W)\n",
        "    it = np.nditer(W, flags=['multi_index'], op_flags=['readwrite'])\n",
        "\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        old_value = W[ix]\n",
        "\n",
        "        W[ix] = old_value + h\n",
        "        fxph = f(W)  # f(x + h)\n",
        "\n",
        "        W[ix] = old_value - h\n",
        "        fxmh = f(W)  # f(x - h)\n",
        "\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h)  # Numerical gradient\n",
        "\n",
        "        W[ix] = old_value  # Reset to original value\n",
        "        it.iternext()\n",
        "\n",
        "    return grad\n"
      ],
      "metadata": {
        "id": "C3zO2kFsKGcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3FanO0mHl5c"
      },
      "source": [
        "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `svm_loss_naive`. You will find it helpful to interleave your new code inside the existing function.\n",
        "\n",
        "To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def svm_loss_naive(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, naive implementation (with loops).\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns:\n",
        "    - loss: single float (the SVM loss)\n",
        "    - dW: gradient of loss with respect to W (same shape as W)\n",
        "    \"\"\"\n",
        "    dW = np.zeros(W.shape)  # Initialize the gradient as zero\n",
        "\n",
        "    num_classes = W.shape[1]  # Number of classes (C)\n",
        "    num_train = X.shape[0]    # Number of training examples (N)\n",
        "    loss = 0.0  # Initialize the loss to zero\n",
        "\n",
        "    for i in range(num_train):\n",
        "        scores = X[i].dot(W)  # Compute the scores for all classes\n",
        "        correct_class_score = scores[y[i]]  # Score of the correct class\n",
        "        num_positive_margins = 0  # Count how many margins > 0 for the current example\n",
        "\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                continue  # Skip the correct class\n",
        "            margin = scores[j] - correct_class_score + 1  # Compute the margin\n",
        "            if margin > 0:  # Only consider positive margins\n",
        "                loss += margin  # Accumulate loss\n",
        "\n",
        "                # Compute gradient: update for the incorrect class j\n",
        "                dW[:, j] += X[i]\n",
        "\n",
        "                # Update the gradient for the correct class y[i]\n",
        "                num_positive_margins += 1\n",
        "\n",
        "        # After looping over all classes, update the correct class gradient\n",
        "        dW[:, y[i]] -= num_positive_margins * X[i]\n",
        "\n",
        "    # Average the loss and gradient over the number of training examples\n",
        "    loss /= num_train\n",
        "    dW /= num_train\n",
        "\n",
        "    # Add regularization to the loss and gradient\n",
        "    loss += reg * np.sum(W * W)\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n"
      ],
      "metadata": {
        "id": "TBqHt7KjiC9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Gradient Checking with Numerical Approximation"
      ],
      "metadata": {
        "id": "u6d940aLhdAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_check_sparse(f, x, analytic_grad, num_checks=15, h=1e-5):\n",
        "    \"\"\"\n",
        "    Sample a few random elements and compute numeric gradient at those points.\n",
        "    f is the loss function, x is the weights, analytic_grad is the gradient\n",
        "    calculated using backprop, and num_checks controls how many points we check.\n",
        "    \"\"\"\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([np.random.randint(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)[0]  # evaluate f(x + h)\n",
        "        x[ix] = oldval - h  # decrement by h\n",
        "        fxmh = f(x)[0]  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)  # the slope\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
        "\n",
        "        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n"
      ],
      "metadata": {
        "id": "nGvn_vu4gY0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have some random W, X, y, and reg values initialized:\n",
        "W = np.random.randn(3073, 10) * 0.0001  # Example weight matrix (D=3073, C=10)\n",
        "X = np.random.randn(500, 3073)  # Example minibatch of data (N=500, D=3073)\n",
        "y = np.random.randint(10, size=500)  # Example labels (N=500)\n",
        "reg = 0.1  # Regularization strength\n",
        "\n",
        "# Define the loss function as a lambda that only returns the loss\n",
        "loss_function = lambda W: svm_loss_naive(W, X, y, reg)\n",
        "\n",
        "# Compute the loss and the gradient\n",
        "loss, grad = svm_loss_naive(W, X, y, reg)\n",
        "\n",
        "# Check the gradients using the function above\n",
        "grad_check_sparse(loss_function, W, grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_co0vVQ8gznU",
        "outputId": "ea979337-d1a1-4e7c-e4a1-5f0284a42339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: 0.239507 analytic: 0.239507, relative error: 1.466135e-09\n",
            "numerical: -0.042512 analytic: -0.042512, relative error: 1.044244e-08\n",
            "numerical: -0.178284 analytic: -0.178284, relative error: 3.682592e-10\n",
            "numerical: -0.138424 analytic: -0.138424, relative error: 3.047098e-09\n",
            "numerical: 0.068152 analytic: 0.068152, relative error: 2.643994e-09\n",
            "numerical: 0.061672 analytic: 0.061672, relative error: 3.121167e-09\n",
            "numerical: 0.164331 analytic: 0.164331, relative error: 1.257979e-09\n",
            "numerical: 0.058115 analytic: 0.058115, relative error: 5.249540e-09\n",
            "numerical: 0.043775 analytic: 0.043775, relative error: 1.986304e-09\n",
            "numerical: -0.044732 analytic: -0.044732, relative error: 2.861127e-09\n",
            "numerical: 0.055962 analytic: 0.055962, relative error: 4.220446e-09\n",
            "numerical: 0.019821 analytic: 0.019821, relative error: 1.688310e-08\n",
            "numerical: -0.085914 analytic: -0.085914, relative error: 4.919156e-09\n",
            "numerical: 0.080779 analytic: 0.080779, relative error: 6.668006e-09\n",
            "numerical: 0.092329 analytic: 0.092329, relative error: 2.785486e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMXiItNcHl5c"
      },
      "source": [
        "Complete the implementation of svm_loss_vectorized, and compute the gradient of the loss function in a vectorized way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def svm_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, vectorized implementation.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns:\n",
        "    - loss: single float (the SVM loss)\n",
        "    - dW: gradient of loss with respect to W (same shape as W)\n",
        "    \"\"\"\n",
        "    # Compute the scores (S = X.W)\n",
        "    scores = X.dot(W)  # Shape (N, C)\n",
        "\n",
        "    # Get the correct class scores (Nx1)\n",
        "    correct_class_scores = scores[np.arange(scores.shape[0]), y].reshape(-1, 1)\n",
        "\n",
        "    # Compute the margins (S - S_correct + 1)\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)  # Shape (N, C)\n",
        "\n",
        "    # Set the margins for the correct class to 0\n",
        "    margins[np.arange(scores.shape[0]), y] = 0\n",
        "\n",
        "    # Compute the loss: sum of all margins, divided by the number of training examples\n",
        "    loss = np.sum(margins) / X.shape[0]\n",
        "\n",
        "    # Add regularization to the loss\n",
        "    loss += reg * np.sum(W * W)\n",
        "\n",
        "    # Gradient computation\n",
        "    # Create a mask where margins > 0\n",
        "    mask = np.zeros(margins.shape)\n",
        "    mask[margins > 0] = 1\n",
        "\n",
        "    # Count the number of positive margins for each example\n",
        "    positive_margin_counts = np.sum(mask, axis=1)  # Shape (N,)\n",
        "\n",
        "    # For the correct class, we subtract the total count of positive margins\n",
        "    mask[np.arange(X.shape[0]), y] = -positive_margin_counts\n",
        "\n",
        "    # Compute the gradient (dW)\n",
        "    dW = X.T.dot(mask) / X.shape[0]\n",
        "\n",
        "    # Add regularization to the gradient\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n"
      ],
      "metadata": {
        "id": "sLj8fRH6i2ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorized Loss Calculation"
      ],
      "metadata": {
        "id": "I6MJI7hRkU_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def svm_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Structured SVM loss function, vectorized implementation.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - reg: (float) regularization strength\n",
        "\n",
        "    Returns:\n",
        "    - loss: Single float (the SVM loss)\n",
        "    - dW: Gradient of loss with respect to W (same shape as W)\n",
        "    \"\"\"\n",
        "    # Get dimensions\n",
        "    num_train = X.shape[0]  # Number of training examples\n",
        "    num_classes = W.shape[1]  # Number of classes\n",
        "\n",
        "    # Compute the scores for all examples (N x C)\n",
        "    scores = X.dot(W)\n",
        "\n",
        "    # Select the correct class scores from scores matrix (Nx1)\n",
        "    correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1)\n",
        "\n",
        "    # Compute the margins (NxC)\n",
        "    margins = np.maximum(0, scores - correct_class_scores + 1)\n",
        "\n",
        "    # Do not consider correct class in margin calculation\n",
        "    margins[np.arange(num_train), y] = 0\n",
        "\n",
        "    # Compute the loss: sum of margins, divided by the number of examples\n",
        "    loss = np.sum(margins) / num_train\n",
        "\n",
        "    # Add regularization to the loss\n",
        "    loss += reg * np.sum(W * W)\n",
        "\n",
        "    # Gradient calculation\n",
        "    # Create a mask of margins > 0\n",
        "    margin_mask = np.zeros(margins.shape)\n",
        "    margin_mask[margins > 0] = 1\n",
        "\n",
        "    # For each training example, we count how many margins > 0 (N,)\n",
        "    row_sum = np.sum(margin_mask, axis=1)\n",
        "\n",
        "    # Subtract in the correct class for each example\n",
        "    margin_mask[np.arange(num_train), y] -= row_sum\n",
        "\n",
        "    # Compute the gradient: X^T.dot(margin_mask) gives the gradient for W\n",
        "    dW = X.T.dot(margin_mask) / num_train\n",
        "\n",
        "    # Add regularization to the gradient\n",
        "    dW += 2 * reg * W\n",
        "\n",
        "    return loss, dW\n"
      ],
      "metadata": {
        "id": "BNaP4pcNkYAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Assuming W, X_dev, y_dev are already defined somewhere in your environment\n",
        "\n",
        "# Measure time for naive implementation\n",
        "tic = time.time()\n",
        "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
        "\n",
        "# Measure time for vectorized implementation\n",
        "tic = time.time()\n",
        "_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time\n",
        "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('difference: %f' % difference)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t4wEDH1kqtM",
        "outputId": "3e5e40da-1506-476e-edee-779f4e6a65e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive loss and gradient: computed in 0.074867s\n",
            "difference: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8cSFOnzHl5d"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WddmfJ4Hl5d"
      },
      "source": [
        "Naive Loss and Gradient Computation Time:\n",
        "\n",
        "Time Taken: The naive implementation of the SVM loss and gradient was computed in approximately 0.075 seconds.\n",
        "This indicates that the naive implementation, which uses loops to calculate the loss and gradients, performed reasonably well for the provided data size.\n",
        "Difference Between Naive and Vectorized Gradients:\n",
        "\n",
        "Difference: The reported difference between the gradients computed using the naive and vectorized implementations is 0.000000.\n",
        "This implies that the two implementations yield identical gradients, suggesting that both implementations are correct and consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJiFxxQ9Hl5e"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
